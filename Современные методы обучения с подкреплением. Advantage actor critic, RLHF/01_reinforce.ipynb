{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkCC1iri2SN"
   },
   "source": [
    "## HW 4: Policy gradient\n",
    "_Reference: based on Practical RL course by YSDA_\n",
    "\n",
    "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
    "\n",
    "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
    "\n",
    "\n",
    "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7UYczVTli2Sb"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "XPKYrIlai2Sf",
    "outputId": "2e044ee7-3baa-4bd7-a214-23b7225a88b5"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75eHkuwTi2Si"
   },
   "source": [
    "# Building the network for Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_TFCmsWi2Sj"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sY2THBWfi2Sl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8_pYr7PZi2Sn"
   },
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits.\n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(state_dim[0],256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,n_actions)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y80qbQFi2Sq"
   },
   "source": [
    "#### Predicting the action probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12PjRu0mi2Sr"
   },
   "source": [
    "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
    "\n",
    "So, here gradient calculation is not needed.\n",
    "\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "\n",
    "Also, `.detach()` can be used instead, but there is a difference:\n",
    "\n",
    "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "d5B5JuXCi2St"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # Use no_grad to suppress gradient calculation.\n",
    "    with torch.no_grad():\n",
    "        #перевод в tensor для model\n",
    "        nn_states=torch.tensor(states,dtype=torch.float32)\n",
    "        logi=model(nn_states)\n",
    "        probs = torch.softmax(logi, dim=-1)\n",
    "        #обратно\n",
    "        return probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Obkl_jCii2Sv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\"\n",
    "print('s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be6AYf8gi2Sw"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8LOUUvnki2Sx"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    states, actions, rewards = [], [], []\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5sdENWJAi2Sz"
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5hLg-3i2S0"
   },
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AoWX9gvai2S0"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session\n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    #G_t = r_t + gamma*G_{t+1}\n",
    "    #то есть для Gt+1=0 так как нет будущ наград\n",
    "    cumulative_rewards=[]\n",
    "    Gt=0\n",
    "    #считаем с конца по формулке\n",
    "    for i in reversed(rewards):\n",
    "        Gt=i+gamma*Gt\n",
    "        #cumulative_rewards.insert(0,Gt) за O(n), так что возможно развернув будет даже быстрее\n",
    "        cumulative_rewards.append(Gt)\n",
    "    return cumulative_rewards[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DX39wcUi2S3",
    "outputId": "9916590d-b093-4c5b-dd84-4ed9ed4b2ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evLt5DJji2S_"
   },
   "source": [
    "### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
    "\n",
    "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
    "\n",
    "$$\n",
    "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
    "$$\n",
    "where $\\lambda$ is the `entropy_coef`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_hLjxTVLi2TB"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Compute the loss for the REINFORCE algorithm.\n",
    "    \"\"\"\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64)  # тут похоже опечатка так как int32 не работает, он наоборот в минимум стремится почему то\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / (cumulative_returns.std() + 1e-8)\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # как по лекции через one-hot, но он медленный немножко, но работают оба\n",
    "    # one_hot= to_one_hot(actions, n_actions)\n",
    "    # log_probs_for_actions=torch.sum(log_probs*one_hot, dim=1)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    # прочитал, что gather будет быстрее правда чтобы он работал, нужно сделать столько же размерностей сколько и у входа\n",
    "    log_probs_for_actions = log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns)\n",
    "    p_loss = -J_hat\n",
    "\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1).mean()\n",
    "    loss = p_loss - entropy_coef * entropy\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1C8ZSizji2TD"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_on_session(states, actions, rewards, optimizer=optimizer, gamma=0.99, entropy_coef=1e-2):\n",
    "    loss = get_loss(states, actions, rewards, gamma, entropy_coef)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-WWsbl5i2TE"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckHj5sXBi2TE",
    "outputId": "017d3bcd-0d01-4632-ed1c-60642792cddf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7688\\3975912250.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:30.160\n",
      "mean reward:56.050\n",
      "mean reward:161.480\n",
      "mean reward:344.590\n",
      "mean reward:815.640\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-2) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg__sQeti2TF"
   },
   "source": [
    "### Watch the video of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "moviepy is not installed, run `pip install \"gymnasium[other]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\gymnasium\\utils\\save_video.py:13\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageSequenceClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageSequenceClip\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'moviepy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave_video\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_video\n\u001b[0;32m      6\u001b[0m env_for_video \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array_list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m env_for_video\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "File \u001b[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\gymnasium\\utils\\save_video.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageSequenceClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageSequenceClip\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m gym\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoviepy is not installed, run `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[other]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcapped_cubic_video_schedule\u001b[39m(episode_id: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The default episode trigger.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    This function will trigger recordings at the episode indices :math:`\\{0, 1, 4, 8, 27, ..., k^3, ..., 729, 1000, 2000, 3000, ...\\}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m        If to apply a video schedule number\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: moviepy is not installed, run `pip install \"gymnasium[other]\"`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "n_actions = env_for_video.action_space.n\n",
    "\n",
    "episode_index = 0\n",
    "step_starting_index = 0\n",
    "\n",
    "obs, info = env_for_video.reset()\n",
    "\n",
    "for step_index in range(800):\n",
    "    probs = predict_probs(np.array([obs]))[0]\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done or step_index == 799:\n",
    "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
    "        frames = env_for_video.render()\n",
    "        os.makedirs(\"videos\", exist_ok=True)\n",
    "        save_video(\n",
    "            frames, \"videos\",\n",
    "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        episode_index += 1\n",
    "        step_starting_index = step_index + 1\n",
    "        obs, info = env_for_video.reset()\n",
    "\n",
    "env_for_video.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus part (no points, just for the interested ones)\n",
    "\n",
    "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
    "\n",
    "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13480\\695723728.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"Acrobo.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Определяем устройство (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Создаём среду\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]  # scalar, not tuple\n",
    "\n",
    "# Модель с двумя выходами: policy logits и value estimate\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, hidden1=256, hidden2=64):\n",
    "        super().__init__()\n",
    "        self.common = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(hidden2, n_actions)   # policy logits\n",
    "        self.value_head = nn.Linear(hidden2, 1)            # state value V(s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.common(x)\n",
    "        policy_logits = self.policy_head(features)\n",
    "        value = self.value_head(features).squeeze(-1)      # [batch] instead of [batch, 1]\n",
    "        return policy_logits, value\n",
    "\n",
    "# model = PolicyValueNetwork(state_dim, n_actions).to(device)\n",
    "\n",
    "try:\n",
    "    model = torch.load(\"Acrobo.pth\", map_location=device)\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_probs(states):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_dim]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "        logits, _ = model(states_tensor)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "def generate_session(env, t_max=500):\n",
    "    states, actions, rewards = [], [], []\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "        #a = np.argmax(action_probs)\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n",
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    cumulative_rewards = []\n",
    "    Gt = 0\n",
    "    for r in reversed(rewards):\n",
    "        Gt = r + gamma * Gt\n",
    "        cumulative_rewards.append(Gt)\n",
    "    return cumulative_rewards[::-1]\n",
    "\n",
    "def get_loss(states, actions, rewards, gamma=0.99, entropy_coef=1e-2, critic_coef=0.5):\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "\n",
    "    # Получаем G_t (Monte-Carlo returns)\n",
    "    G_t = torch.tensor(get_cumulative_rewards(rewards, gamma), dtype=torch.float32).to(device)\n",
    "\n",
    "    # Прогоняем через модель: получаем logits и V(s)\n",
    "    logits, values = model(states)  # values: [T], G_t: [T]\n",
    "\n",
    "    # === Actor loss (policy) ===\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    log_probs_for_actions = log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Advantage = G_t - V(s_t)\n",
    "    advantages = G_t - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    actor_loss = -(log_probs_for_actions * advantages).mean()\n",
    "\n",
    "    # === Critic loss (value function) ===\n",
    "    critic_loss = nn.functional.mse_loss(values, G_t)\n",
    "\n",
    "    # === Entropy bonus ===\n",
    "    entropy = -(probs * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "    # === Total loss ===\n",
    "    total_loss = actor_loss + critic_coef * critic_loss - entropy_coef * entropy\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: mean reward = -106.490\n",
      "Episode 2: mean reward = -108.100\n",
      "Episode 3: mean reward = -99.470\n",
      "Episode 4: mean reward = -107.910\n",
      "Episode 5: mean reward = -119.870\n",
      "Episode 6: mean reward = -103.190\n",
      "Episode 7: mean reward = -114.910\n",
      "Episode 8: mean reward = -100.360\n",
      "Episode 9: mean reward = -102.340\n",
      "Episode 10: mean reward = -98.800\n",
      "Episode 11: mean reward = -109.630\n",
      "Episode 12: mean reward = -105.530\n"
     ]
    }
   ],
   "source": [
    "def train_on_session(states, actions, rewards, optimizer, gamma=0.99, entropy_coef=1e-2):\n",
    "    loss = get_loss(states, actions, rewards, gamma, entropy_coef)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(np.sum(rewards))\n",
    "\n",
    "# Обучение\n",
    "for i in range(500):\n",
    "    session_rewards = []\n",
    "    for _ in range(100):\n",
    "        states, actions, rewards = generate_session(env)\n",
    "        total_reward = train_on_session(states, actions, rewards, optimizer, entropy_coef=1e-4)\n",
    "        session_rewards.append(total_reward)\n",
    "\n",
    "    mean_reward = np.mean(session_rewards)\n",
    "    print(f\"Episode {i+1}: mean reward = {mean_reward:.3f}\")\n",
    "\n",
    "    if mean_reward > -80:\n",
    "        print(\"You Win!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"Acrobo.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env_for_video = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array_list\")\n",
    "n_actions = env_for_video.action_space.n\n",
    "\n",
    "episode_index = 0\n",
    "step_starting_index = 0\n",
    "\n",
    "obs, info = env_for_video.reset()\n",
    "\n",
    "for step_index in range(800):\n",
    "    probs = predict_probs(np.array([obs]))[0]\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done or step_index == 799:\n",
    "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
    "        frames = env_for_video.render()\n",
    "        os.makedirs(\"videos\", exist_ok=True)\n",
    "        save_video(\n",
    "            frames, \"videos\",\n",
    "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        episode_index += 1\n",
    "        step_starting_index = step_index + 1\n",
    "        obs, info = env_for_video.reset()\n",
    "\n",
    "env_for_video.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
